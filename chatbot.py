# -*- coding: utf-8 -*-
"""ChatBOT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RBqIETAK-VXMa84Pg-lMjbB2n2XrPSTR
"""

!pip -q install langchain openai tiktoken chromadb

!pip install langchain
!pip install langchain_community
!pip install langchain-text-splitters

import os
from getpass import getpass
os.environ["OPENAI_API_KEY"] = getpass()

!pip install pypdf

!pip install langchain
!pip install chromadb
!pip install openai
!pip install pymupdf
!pip install langchain_community

from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.document_loaders import PyPDFLoader
from langchain.chains.qa_with_sources import load_qa_with_sources_chain
from langchain.llms import OpenAI
# Initialize OpenAI embeddings
embeddings = OpenAIEmbeddings(api_key="sk-your-openai-api-key")

from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.document_loaders import PyPDFLoader
from langchain.chains.qa_with_sources import load_qa_with_sources_chain
from langchain.llms import OpenAI

# Initialize OpenAI embeddings
embeddings = OpenAIEmbeddings(api_key="sk-proj-wQitXMY83BAqqnfRwFAST3BlbkFJLYgiC2p4LSVHlKJjWOL1")

# Initialize Chroma vector store
vector_store = Chroma(embedding_function=embeddings)

# Load a single PDF document
pdf_loader = PyPDFLoader("/content/sample_data/data/india.pdf")
documents = pdf_loader.load()

# Split text into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
chunks = text_splitter.split_documents(documents)

# Add document chunks to vector store
vector_store.add_documents(chunks)

# Initialize LLM
llm = OpenAI(temperature=0)

# Create RetrievalQA chain with combine_documents_chain
chain = load_qa_with_sources_chain(llm, chain_type="stuff")
retrieval_qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=vector_store.as_retriever())

# Perform a query
query = "Your query here"
response = retrieval_qa({"query": query})
print(response)

# Initialize Chroma vector store
vector_store = Chroma(embedding_function=embeddings)

# Load a single PDF document
pdf_loader = PyPDFLoader("/content/sample_data/data/india.pdf")
documents = pdf_loader.load()

# Split text into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
chunks = text_splitter.split_documents(documents)

# Add document chunks to vector store
vector_store.add_documents(chunks)

# Initialize LLM
llm = OpenAI(temperature=0)

# Create RetrievalQA chain with combine_documents_chain
chain = load_qa_with_sources_chain(llm, chain_type="stuff")
retrieval_qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=vector_store.as_retriever())

# Perform a query
query = "Your query here"
response = retrieval_qa({"query": query})
print(response)

from google.colab import drive
drive.mount('/content/drive')



loader = PyPDFLoader("/content/sample_data/data/india.pdf")
documents = loader.load()

print(documents)

text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)

texts = text_splitter.split_documents(documents)

pip install sentence_transformers

from langchain.vectorstores import Chroma
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.document_loaders import PyPDFLoader

# Initialize Sentence Transformers embeddings
embeddings = SentenceTransformer('all-MiniLM-L6-v2')

# Initialize Chroma vector store, passing the 'embeddings' object directly
vector_store = Chroma(embedding_function=embeddings)  # Pass the embeddings object, not a function

# Load a single PDF document
loader = PyPDFLoader("/content/sample_data/data/india.pdf")
documents = loader.load()

# Split text into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
texts = text_splitter.split_documents(documents)

# Add document chunks to vector store
vector_store.add_documents(texts)

# Create RetrievalQA chain
retrieval_qa = RetrievalQA(retriever=vector_store.as_retriever())

# Perform a query
query = "Your query here"
response = retrieval_qa({"query": query})
print(response)

len(texts)

persist_directory = 'db'
embedding = OpenAIEmbeddings()

vectordb = Chroma.from_documents(documents=texts,
                                 embedding=embedding,
                                 persist_directory=persist_directory)

vectordb = Chroma(persist_directory=persist_directory,
                  embedding_function=embedding)
vectordb.persist()

from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(
    temperature=0,
    model_name='gpt-3.5-turbo'
)

from langchain import PromptTemplate

prompt_template = PromptTemplate(
    input_variables=["context"],
    template="""
1. What is the essence of rural Rajasthan?
{context}

2. What is special about Narlai?
Narlai, situated midway between Jodhpur & Udaipur, is known for its 17th-century villa, Ravla Narlai, now converted into a heritage hotel. A stay here allows visitors to explore the surrounding village.

3. What is Luni known for?
Luni is a hub of activity with many artisans fashioning metal, clay, and wood into intricate designs, demonstrating skills learned over centuries. Fort Chanwa of Luni, an exceptional example of elegance and symmetry in Indian architecture, has been converted into a heritage hotel.
"""
)

# You can then use prompt_template.format(context="Your context here") to get the formatted output
formatted_output = prompt_template.format(context="Your context here")
print(formatted_output)

prompt_template.format(context = "What is special about Narlai?")

from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.document_loaders import TextLoader, PyPDFDirectoryLoader
from langchain import PromptTemplate
from langchain.llms import OpenAI
import os

# Initialize OpenAI embeddings
try:
    embeddings = OpenAIEmbeddings(api_key="")
    print("OpenAI embeddings initialized successfully.")
except Exception as e:
    print(f"Error initializing OpenAI embeddings: {e}")

# Initialize Chroma vector store
try:
    vector_store = Chroma(embedding_function=embeddings)
    print("Chroma vector store initialized successfully.")
except Exception as e:
    print(f"Error initializing Chroma vector store: {e}")

# Load documents
pdf_directory_path = "/content/sample_data/data/"
pdf_file_name = "india.pdf"
pdf_file_path = os.path.join(pdf_directory_path, pdf_file_name)

# Debug: Verify paths
if not os.path.exists(pdf_file_path):
    print(f"PDF file not found at path: {pdf_file_path}")
else:
    print(f"PDF file found at path: {pdf_file_path}")
    try:
        pdf_loader = PyPDFDirectoryLoader(directory_path=pdf_directory_path)
        text_loader = TextLoader(file_path="path_to_your_text_file")
        pdf_documents = pdf_loader.load()
        text_documents = text_loader.load()
        documents = pdf_documents + text_documents
        print(f"Loaded {len(documents)} documents")
    except Exception as e:
        print(f"Error loading documents: {e}")

# Split text into chunks
try:
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    chunks = text_splitter.split_documents(documents)
    print(f"Split documents into {len(chunks)} chunks")
except Exception as e:
    print(f"Error splitting documents: {e}")

# Add document chunks to vector store
try:
    vector_store.add_documents(chunks)
    print("Added document chunks to vector store")
except Exception as e:
    print(f"Error adding document chunks to vector store: {e}")

# Create a prompt template (this is a placeholder, adjust as needed)
prompt_template = PromptTemplate(input_variables=["query"], template="Your prompt template here with {query}")

# Initialize the OpenAI LLM
try:
    llm = OpenAI(api_key="sk-proj-omP2DNRc0hZq7Q2UmOlfT3BlbkFJFqYTaWlxNagFM6dw8Imc")
    print("LLM initialized successfully.")
except Exception as e:
    print(f"Error initializing LLM: {e}")

# Create RetrievalQA chain
try:
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_store.as_retriever(),
        chain_type_kwargs={"prompt": prompt_template},
        return_source_documents=True
    )
    print("RetrievalQA chain created successfully.")
except Exception as e:
    print(f"Error creating RetrievalQA chain: {e}")

# Perform a query
try:
    query = "What is special about Narlai?"
    response = qa_chain({"query": query})
    print("Query response:", response)
except Exception as e:
    print(f"Error performing query: {e}")

import pprint

from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.document_loaders import TextLoader, PyPDFDirectoryLoader
from langchain import PromptTemplate
from langchain.llms import OpenAI
import os
import pprint

# Initialize OpenAI embeddings
try:
    embeddings = OpenAIEmbeddings(api_key="sk-proj-omP2DNRc0hZq7Q2UmOlfT3BlbkFJFqYTaWlxNagFM6dw8Imc")
    print("OpenAI embeddings initialized successfully.")
except Exception as e:
    print(f"Error initializing OpenAI embeddings: {e}")

# Initialize Chroma vector store
try:
    vector_store = Chroma(embedding_function=embeddings)
    print("Chroma vector store initialized successfully.")
except Exception as e:
    print(f"Error initializing Chroma vector store: {e}")

# Load documents
pdf_directory_path = "/content/sample_data/data/"
pdf_file_name = "india19.pdf"
pdf_file_path = os.path.join(pdf_directory_path, pdf_file_name)

# Debug: Verify paths
if not os.path.exists(pdf_file_path):
    print(f"PDF file not found at path: {pdf_file_path}")
else:
    print(f"PDF file found at path: {pdf_file_path}")
    try:
        pdf_loader = PyPDFDirectoryLoader(directory_path=pdf_directory_path)
        pdf_documents = pdf_loader.load()
        print(f"Loaded {len(pdf_documents)} documents from PDF")
    except Exception as e:
        print(f"Error loading PDF documents: {e}")

# Split text into chunks
try:
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    chunks = text_splitter.split_documents(pdf_documents)
    print(f"Split documents into {len(chunks)} chunks")
except Exception as e:
    print(f"Error splitting documents: {e}")

# Add document chunks to vector store
try:
    vector_store.add_documents(chunks)
    print("Added document chunks to vector store")
except Exception as e:
    print(f"Error adding document chunks to vector store: {e}")

# Create a prompt template (this is a placeholder, adjust as needed)
prompt_template = PromptTemplate(input_variables=["query"], template="Your prompt template here with {query}")

# Initialize the OpenAI LLM
try:
    llm = OpenAI(api_key="sk-proj-omP2DNRc0hZq7Q2UmOlfT3BlbkFJFqYTaWlxNagFM6dw8Imc")
    print("LLM initialized successfully.")
except Exception as e:
    print(f"Error initializing LLM: {e}")

# Create RetrievalQA chain
try:
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_store.as_retriever(),
        chain_type_kwargs={"prompt": prompt_template},
        return_source_documents=True
    )
    print("RetrievalQA chain created successfully.")
except Exception as e:
    print(f"Error creating RetrievalQA chain: {e}")

# Now, use qa_chain to perform a query related to Narlai
try:
    query = "What is special about Narlai? Narlai, situated midway between Jodhpur & Udaipur, is known for its 17th-century villa, Ravla Narlai, now converted into a heritage hotel. A stay here allows visitors to explore the surrounding village."
    response = qa_chain({"query": query})
    pprint.pprint(response)
except Exception as e:
    print(f"Error performing query: {e}")

from pprint import pprint
from transformers import DistilBertModel, DistilBertTokenizer
from langchain.chains import RetrievalQA

# Initialize DistilBERT model and tokenizer
model_name = "distilbert-base-nli-stsb-mean-tokens"
tokenizer = DistilBertTokenizer.from_pretrained(model_name)
model = DistilBertModel.from_pretrained(model_name)

# Define a function to encode text using DistilBERT
def encode_text(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    outputs = model(**inputs)
    embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling
    return embeddings.detach().numpy()

# Initialize qa_chain with RetrievalQA using BERT embeddings
qa_chain = RetrievalQA(retriever=encode_text)

# Example usage
query = "What makes Narlai special? Discover its 17th-century villa, Ravla Narlai, now a heritage hotel, offering a glimpse into the surrounding village."
response = qa_chain(query)

# Print the response using pprint for nice formatting
pprint(response)